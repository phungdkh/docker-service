input {
    
    rabbitmq {
        type => "command"
        host => "rabbitmq"
        queue => "command_queue"
        exchange => "command_exchange"
        exchange_type => "direct"
        heartbeat => 30
        durable => true
        key => "command"
        user => "${RABBITMQ_USERNAME}"
        password => "${RABBITMQ_PASSWORD}"
    }
}

output {
      # Must use type to filter incoming, else 3 messages (event, command, analytic)
      # will be written to MongoDB. Reason is that Docker image config does not 
      # support running in multiple pipeline mode. It means that all defined 
      # output commands get a copy of incoming message regardless of what queue was source
      # of message.... Quite annoying :( 

      if [type] == "command" {
            # stdout { codec => rubydebug }

            elasticsearch {
                hosts => "elasticsearch:9200"
                index => "datahub-%{+YYYY.MM.dd}"
            }

            mongodb {
                collection => "command"
                database => "datahub"
                uri => "mongodb://mongodb:27017/datahub"        
                codec => "json"
            }

            if [routing][action] == "cxid.crud.create.user" {
                rabbitmq {
                    id => "rabbit-snapshot-id"
                    key => "snapshot"
                    exchange => "processor_exchange"
                    exchange_type => "direct"
                    host => "rabbitmq"
                    port => 5672
                    durable => true
                    persistent => true
                    user => "${RABBITMQ_USERNAME}"
                    password => "${RABBITMQ_PASSWORD}"
                }
            }
        }
    }